---
marp: true
theme: gaia
paginate: true

---
# 線形回帰 (Linear Regression)

---

## 学習目標

- **損失関数**とその動作について説明できる。
- **勾配降下法**が最適なモデルパラメータをどのように見つけるかを理解する。
- 線形モデルを効率的に訓練するための**ハイパーパラメータ**の調整方法を理解する。

---

## 線形回帰の基本

線形回帰は、特徴量（特徴）とラベル（結果）の関係を見つけるための統計的手法です。  
機械学習においては、線形回帰は特徴量とラベル間の関係をモデル化します。

---

## 例：車の燃費予測

車の重さを基に燃費（ガロン当たりのマイル数）を予測するモデルを考えます。

<style scoped>
  table { table-layout: fixed; width: 100%; display:table; font-size: 24px; }
</style>

| 車の重さ (1000ポンド) | 燃費 (ガロン当たりのマイル数) |
|----------------------|---------------------------|
| 3.5                  | 18                        |
| 3.69                 | 15                        |
| 3.44                 | 18                        |
| 3.43                 | 16                        |
| 4.34                 | 15                        |
| 4.42                 | 14                        |
| 2.37                 | 24                        |

---

## グラフによる可視化

このデータをグラフにプロットすると、重さが増すにつれて燃費が減少する傾向が見られます。

---

## モデルの作成

データ点に最適な直線を引くことでモデルを作成します。  
この直線の式は、次のように表現できます。

---

## 線形回帰の数式

線形回帰モデルの数式は以下のように表されます。

$$ y' = b + w_1 x_1 $$

- **y'**: 予測されるラベル（出力）
- **b**: バイアス（y切片）
- **w_1**: 特徴量の重み（傾き）
- **x_1**: 特徴量（入力）

---

## モデルの計算例

例えば、上記のデータを使って、重さと燃費の関係を予測します。  
バイアスは30、重みは-3.6で、次の式を得ます。

$$ \text{予測燃費} = 30 - 3.6 \times \text{重さ} $$

これにより、4,000ポンドの車の予測燃費は15.6マイル/ガロンとなります。

---

## 複数の特徴量を使ったモデル

複数の特徴量を使用する場合、モデルは次のように書けます。

$$ y' = b + w_1 x_1 + w_2 x_2 + \cdots + w_n x_n $$

例として、以下の特徴量が車の燃費に影響を与える可能性があります：
- エンジンの排気量
- 加速性能
- シリンダー数
- 馬力

---

## 訓練時の更新

訓練中に更新されるのは、**バイアス**と**重み**です。これらは、最適な予測を行うために勾配降下法で調整されます。

---

## 練習問題

線形回帰モデルの訓練時に更新される部分は何ですか？

- 特徴量の値
- 予測結果
- バイアスと重み

---
# 線形回帰: 損失関数 (Loss)

---

## 損失関数とは

損失関数は、モデルの予測がどれだけ間違っているかを示す数値的な指標です。  
損失は、モデルの予測と実際のラベルとの間の距離を測定します。  
モデルの訓練の目的は、損失を最小化し、できる限り低い値にすることです。

---

## 損失の視覚化

以下の図では、データ点からモデルに向かって引かれた矢印が損失を示しています。  
矢印は、モデルの予測が実際の値からどれだけ離れているかを示しています。

---

## 損失の距離

統計学と機械学習において、損失は予測値と実際の値との差を測定します。  
損失は、値の方向ではなく距離に焦点を当てます。  
例えば、モデルが2を予測し、実際の値が5だった場合、損失は負の値には関心がなく、差の絶対値（3）を重視します。

---

## 損失の種類

線形回帰では、主に以下の4種類の損失関数があります。

<style scoped>
  table { table-layout: fixed; width: 100%; display:table; font-size: 24px; }
</style>

| 損失の種類           | 定義                                                      | 数式                                |
|----------------------|-----------------------------------------------------------|-------------------------------------|
| **L1損失**           | 予測値と実際の値の絶対差の合計                            | $L_1 = \sum |y - \hat{y}|$            |
| **平均絶対誤差 (MAE)**| 複数の例にわたるL1損失の平均                            | $MAE = \frac{1}{n} \sum |y - \hat{y}|$ |
| **L2損失**           | 予測値と実際の値の差の二乗の合計                        | $L_2 = \sum (y - \hat{y})^2$        |
| **平均二乗誤差 (MSE)**| 複数の例にわたるL2損失の平均                            | $MSE = \frac{1}{n} \sum (y - \hat{y})^2$ |

---

## L1損失 vs L2損失

L1損失とL2損失の違いは、差を二乗するかどうかにあります。  
予測と実際の値の差が大きい場合、二乗すると損失がさらに大きくなります。  
逆に、差が小さい場合（1未満）では、二乗によって損失が小さくなります。

---

## 損失計算の例

前回の最適化された直線を使って、単一のデータポイントに対するL2損失を計算します。  
予測が2,370ポンドの車に対して21.5マイル/ガロン、実際の値が24マイル/ガロンの場合：

<style scoped>
  table { table-layout: fixed; width: 100%; display:table; font-size: 24px; }
</style>

| 値              | 数式                            | 結果   |
|-----------------|---------------------------------|--------|
| 予測値         | $21.5$                          |        |
| 実際の値       | $24$                            |        |
| L2損失         | $(24 - 21.5)^2 = 6.25$          | 6.25   |

この例では、単一のデータポイントに対するL2損失は6.25です。

---

## 損失関数の選択

MAEとMSEの選択は、データセットや特定の予測値の取り扱いに依存します。  
例えば、車の重さが通常2000〜5000ポンド、燃費が8〜50マイル/ガロンであれば、外れ値（例えば8,000ポンドの車や100マイル/ガロンの車）を考慮する必要があります。

---

## 外れ値と損失関数

- **MSE**は外れ値に対して強いペナルティを課すため、外れ値に引き寄せられたモデルになります。
- **MAE**は外れ値に対してあまり影響を受けず、一般的なデータに近いモデルになります。

---

## MSEとMAEのモデル比較

- **MSE**: モデルは外れ値に近づき、他のデータ点からは遠ざかります。
- **MAE**: モデルは外れ値から遠ざかり、他のデータ点に近づきます。

---

## 理解度チェック

次の2つのプロットを比較してください：

- 10個のデータ点があり、1つの直線が6点を通り、2点が直線の上に1単位、2点が下に1単位ずれている。
- 10個のデータ点があり、1つの直線が8点を通り、1点が直線の上に2単位、1点が下に2単位ずれている。

どちらのデータセットがより高い**平均二乗誤差 (MSE)**を持っていますか？

- 右側のデータセット
- 左側のデータセット

---
# 線形回帰: 勾配降下法 (Gradient Descent)

---

## 勾配降下法とは

勾配降下法は、最小の損失を持つモデルの重みとバイアスを繰り返し求める数学的手法です。  
この手法は、以下の手順を繰り返して最適な重みとバイアスを見つけます。

1. 初期化された重みとバイアスで損失を計算する。
2. 損失を減少させる方向に重みとバイアスを移動させる。
3. 移動後、再度損失を計算してその方向を調整する。

---

## モデルの収束と損失曲線

訓練中、損失曲線を見てモデルが収束しているかを確認します。  
以下は典型的な損失曲線です。y軸は損失、x軸は繰り返し回数を示します。

---

## 損失曲線の例

最初は急激に損失が減少し、その後緩やかに減少します。  
約1,000回目のイテレーションで、モデルは収束したと判断できます。

---

## 訓練過程でのモデルの状態

モデルの訓練過程をいくつかの段階（初期、中間、終了）で可視化することで、  
重みとバイアスの更新が損失を減少させ、収束に至る過程を理解できます。

- **初期状態（第2回目）**: 損失が大きく、予測が不正確。
- **中間状態（第400回目）**: より良いモデルに近づく。
- **最終状態（第1,000回目）**: 収束し、最小の損失を達成。

---

## 理解度チェック

勾配降下法の役割は次のうちどれですか？

1. 勾配降下法はモデル訓練中に使用する損失関数（L1やL2など）を決定します。
2. 勾配降下法は反復的に重みとバイアスを更新し、損失を最小化する最適なモデルを見つけます。
3. 勾配降下法は外れ値を削除し、モデルの予測精度を向上させます。

---
# 線形回帰: ハイパーパラメータ (Hyperparameters)

---

## ハイパーパラメータとは

ハイパーパラメータは訓練プロセスのさまざまな側面を制御する変数です。  
主なハイパーパラメータには次の3つがあります：

- 学習率 (Learning Rate)
- バッチサイズ (Batch Size)
- エポック数 (Epochs)

パラメータはモデル自身に含まれ、訓練中に計算される変数です。

---

## 学習率 (Learning Rate)

学習率は、モデルがどれくらいの速さで収束するかを決める浮動小数点の値です。  
学習率が小さすぎると収束に時間がかかり、大きすぎると収束せずに損失が上下してしまいます。

- 学習率は勾配降下法で重みとバイアスをどれだけ変更するかを決定します。
- 目標は、収束までの回数が少なく、効率的に学習できる最適な学習率を選ぶことです。

---

## 学習率が小さすぎる場合

学習率が小さすぎると、収束には非常に多くの反復が必要になります。  
この場合、損失曲線は緩やかな下降となり、改善が非常に遅くなります。

---

## 学習率が大きすぎる場合

学習率が大きすぎると、モデルが収束せず、損失が上下に揺れ動きます。  
損失曲線は、上下に揺れるパターンが見られます。

---

## 理想的な学習率

最適な学習率は問題に依存します。  
一般的には0.01などの小さい値から始めるのが良いとされています。

---

## バッチサイズ (Batch Size)

バッチサイズは、モデルが重みとバイアスを更新する前に処理するデータの数を指します。  
データセットが非常に大きい場合、全データを一度に処理するのは実用的ではありません。

---

### よく使われる手法
- **確率的勾配降下法 (SGD)**: 一度に1つの例を使用して重みとバイアスを更新します。
  - ただし、ノイズが多くなるため、収束に時間がかかります。
- **ミニバッチ確率的勾配降下法 (Mini-batch SGD)**: 複数の例を使用して重みとバイアスを更新し、ノイズを減らします。

---

## エポック数 (Epochs)

エポック数は、訓練セットの全データを1回処理する回数を指します。  
例えば、1,000のデータポイントがあり、バッチサイズが100の場合、1エポックを完了するには10回の反復が必要です。

多くのエポックを設定するほど、モデルは改善しますが、訓練時間も長くなります。

---

## バッチタイプとエポック数の関係

- **フルバッチ**: データセットのすべての例を見た後、重みとバイアスを1回更新します。
- **確率的勾配降下法**: 各データポイントを使って、重みとバイアスを毎回更新します。
- **ミニバッチSGD**: 各バッチのデータを使って、1回の反復ごとに重みとバイアスを更新します。

---

## 理解度チェック

### 1. ミニバッチSGDを使用する際の最適なバッチサイズは？
- 100個の例を1バッチにする
- 10個の例を1バッチにする
- データセットに依存する

---

### 2. 以下のうち、どれが正しいですか？
- 学習率を2倍にすると訓練が遅くなる。
- データに外れ値が多い場合、大きなバッチサイズは不適切。

---
# ロジスティック回帰 (Logistic Regression)

---

## 学習目標

- ロジスティック回帰の使用例を識別できるようになる。
- ロジスティック回帰モデルが確率を計算するためにシグモイド関数をどのように利用するかを説明できるようになる。
- 線形回帰とロジスティック回帰の違いを比較できるようになる。
- ロジスティック回帰が二乗損失の代わりに対数損失を使用する理由を説明できるようになる。
- ロジスティック回帰モデルを訓練する際の正則化の重要性を説明できるようになる。

---

## ロジスティック回帰の概要

ロジスティック回帰は、確率を計算するために使用される効率的な手法です。  
出力される確率は次のように利用できます：

- そのまま確率として使用：  
  例: スパム判定の確率が0.932 → 93.2%の確率でスパム。

- 二値分類に変換：  
  例: True/False, スパム/非スパム。

---

## 線形出力を確率に変換

ロジスティック回帰モデルの線形部分は次のように表されます：

$$
z = w_1x_1 + w_2x_2 + \cdots + w_nx_n + b
$$

- \(z\): 線形出力（log-odds）
- \(b\): バイアス（切片）
- \(w\): 学習された重み
- \(x\): 特徴量（入力）

---

### シグモイド関数の数式

線形出力 \(z\) はシグモイド関数に入力され、確率 \(y'\) が得られます：

$$
y' = \frac{1}{1 + e^{-z}}
$$

- \(y'\): ロジスティック回帰モデルの出力（確率）

---

## 線形出力から確率への変換

次のように線形関数をシグモイド関数で変換します：

- 線形関数 \(z = 2x + 5\) の場合  
  例えば、\(z = -10\) なら、シグモイド関数の出力は \(y' = 0.00004\) です。  
  \(z = 0\) なら \(y' = 0.5\)、\(z = 5\) なら \(y' = 0.9933\) になります。

---